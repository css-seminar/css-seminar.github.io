# Harini Suresh / <01/30/2025> Preparation

## My expectations

- Dimension 1 (scope): I expect this talk to be more about governance, design and community power than model architecture details.
- Dimension 2 (evidence): I expect real case studies  (journalism and activist collaborations) rather than purely theoretical arguments.
- Most useful to me: concrete ways to make “community control” real (decision rights, data control, accountability), especially when the model is large and general purpose.
- Big picture vs details: I expect to understand the big picture,  plus some practical details about what participation actually looks like day-to-day (who participates, how decisions get made, what breaks).

## Jargon look up

- participatory ML
- AI governance (who sets rules, who enforces)
- domain specificity vs general purpose systems
- local agency (who has decision power locally)
- cooperatively controlled model
- counterdata


## Paper summary
### Paper 1
Citation: Tseng, E., Young, M., Aubin Le Quéré, M., Rinehart, A., & Suresh, H. (2025). "Ownership, Not Just Happy Talk": Co-Designing a Participatory Large Language Model for Journalism  (ACM / arXiv). :contentReference[oaicite:4]{index=4}  
Key understanding: Journalists want tools but face power/legal issues with commercial LLMs. The paper surfaces tensions at multiple levels (org + daily work) and sketches what a journalist controlled LLM would require (governance , workflows and  incentives).  
Question for speaker: What’s the hardest “non-technical” bottleneck for a cooperatively controlled journalism LLM: funding, governance, data rights, labor politics, or something else?

### Paper 2
Citation: Suresh, H. (2022). Towards Intersectional Feminist and Participatory ML  (FAccT) :contentReference[oaicite:5]{index=5}  
Key understanding: A feminist or participatory ML approach treats power, harms, and who gets to define success as central, not an afterthought. It pushes for methods that include affected communities and reflect intersectional impacts.  
Question for speaker: What evaluation signals do you trust for “community benefit” or “agency” (beyond accuracy and benchmarks), and how do you avoid participation becoming tokenistic?
